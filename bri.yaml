name: PyTorch Yelp Simple NN Training
description: Simple PyTorch model training on Yelp Review Full dataset via Elyra
inputs:
  - name: epochs
    type: Integer
    default: 2
  - name: batch_size
    type: Integer
    default: 64
  - name: vocab_size
    type: Integer
    default: 2000
  - name: max_train_samples
    type: Integer
    default: 10000
outputs:
  - name: model_output
    type: File
  - name: train_log
    type: File
implementation:
  container:
    image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
    command:
      - bash
      - -lc
      - |
        set -e
        echo "Activating training..."
        pip install --quiet datasets scikit-learn
        python - << 'PYCODE'
        import sys, torch, torch.nn as nn, torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        from datasets import load_dataset
        from sklearn.feature_extraction.text import CountVectorizer
        import argparse, numpy as np

        parser = argparse.ArgumentParser()
        parser.add_argument("--epochs", type=int, required=True)
        parser.add_argument("--batch_size", type=int, required=True)
        parser.add_argument("--vocab_size", type=int, required=True)
        parser.add_argument("--max_train_samples", type=int, required=True)
        parser.add_argument("--model_output", type=str, required=True)
        parser.add_argument("--train_log", type=str, required=True)
        args = parser.parse_args()

        sys.stdout = open(args.train_log, "w")
        sys.stderr = sys.stdout

        print("Loading data...", flush=True)
        ds = load_dataset("yelp_review_full", split=f"train[:{args.max_train_samples}]")
        X = CountVectorizer(max_features=args.vocab_size).fit_transform(ds["text"]).toarray()
        y = np.array(ds["label"])
        dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long))
        loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

        class SimpleNN(nn.Module):
          def __init__(self, input_dim, n_classes):
            super().__init__()
            self.fc1 = nn.Linear(input_dim, 256)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(256, n_classes)
          def forward(self, x):
            return self.fc2(self.relu(self.fc1(x)))

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = SimpleNN(args.vocab_size, len(set(y))).to(device)
        optim = optim.Adam(model.parameters(), lr=1e-3)
        loss_fn = nn.CrossEntropyLoss()

        for epoch in range(args.epochs):
          total_loss, correct, total = 0, 0, 0
          for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            optim.zero_grad()
            preds = model(xb)
            loss = loss_fn(preds, yb); loss.backward(); optim.step()
            total_loss += loss.item() * xb.size(0)
            correct += (preds.argmax(1) == yb).sum().item()
            total += xb.size(0)
          print(f"Epoch {epoch+1}/{args.epochs} â€” loss={(total_loss/total):.4f}, acc={(correct/total):.4f}", flush=True)

        torch.save(model.state_dict(), args.model_output)
        print("Model saved to", args.model_output, flush=True)
        PYCODE
args:
  - --epochs
    - { inputValue: epochs }
  - --batch_size
    - { inputValue: batch_size }
  - --vocab_size
    - { inputValue: vocab_size }
  - --max_train_samples
    - { inputValue: max_train_samples }
  - --model_output
    - { outputPath: model_output }
  - --train_log
    - { outputPath: train_log }
