name: PyTorch Simple NN Training on Yelp
description: Trains a simple PyTorch neural network model for text classification on Yelp Review Full dataset (inline component).

inputs:
  - name: dataset_name
    type: String
    description: Hugging Face dataset name (e.g. yelp_review_full)
  - name: output_dir
    type: String
    description: Directory to save all model output and logs

outputs:
  - name: trained_model
    type: Directory
    description: Directory containing trained PyTorch model file (.pt) and vectorizer
  - name: training_logs
    type: Directory
    description: Directory containing training logs

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install --quiet torch datasets scikit-learn
        python3 -u - <<'EOF'
        import argparse, os, shutil
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        from datasets import load_dataset
        from sklearn.feature_extraction.text import CountVectorizer
        import numpy as np

        parser = argparse.ArgumentParser()
        parser.add_argument("--dataset_name", type=str, required=True)
        parser.add_argument("--output_dir", type=str, required=True)
        parser.add_argument("--trained_model", type=str, required=True)
        parser.add_argument("--training_logs", type=str, required=True)
        args = parser.parse_args()

        os.makedirs(args.output_dir, exist_ok=True)
        os.makedirs(args.trained_model, exist_ok=True)
        os.makedirs(args.training_logs, exist_ok=True)

        # --- Load and preprocess dataset ---
        dataset = load_dataset(args.dataset_name, split="train[:3000]")
        texts = dataset["text"]
        labels = dataset["label"]
        num_classes = len(set(labels))

        # --- Text vectorization ---
        vectorizer = CountVectorizer(max_features=2000)
        X = vectorizer.fit_transform(texts).toarray()
        y = np.array(labels)

        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.long)
        data = TensorDataset(X_tensor, y_tensor)
        loader = DataLoader(data, batch_size=64, shuffle=True)

        # --- Simple NN definition ---
        class SimpleNN(nn.Module):
            def __init__(self, input_dim, num_classes):
                super().__init__()
                self.fc1 = nn.Linear(input_dim, 256)
                self.relu = nn.ReLU()
                self.fc2 = nn.Linear(256, num_classes)
            def forward(self, x):
                return self.fc2(self.relu(self.fc1(x)))

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = SimpleNN(X.shape[1], num_classes).to(device)
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        loss_fn = nn.CrossEntropyLoss()

        # --- Train loop ---
        log_file = os.path.join(args.training_logs, "train.log")
        with open(log_file, "w") as log:
            for epoch in range(2):
                total_loss, correct, total = 0, 0, 0
                for xb, yb in loader:
                    xb, yb = xb.to(device), yb.to(device)
                    optimizer.zero_grad()
                    preds = model(xb)
                    loss = loss_fn(preds, yb)
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item() * xb.size(0)
                    correct += (preds.argmax(1) == yb).sum().item()
                    total += xb.size(0)
                acc = correct / total
                avg_loss = total_loss / total
                log.write(f"Epoch {epoch+1}/2 â€” loss={avg_loss:.4f}, acc={acc:.4f}\n")
                log.flush()

        # --- Save model and vectorizer ---
        torch.save(model.state_dict(), os.path.join(args.trained_model, "model.pt"))
        import joblib
        joblib.dump(vectorizer, os.path.join(args.trained_model, "vectorizer.pkl"))

        # Optionally, copy any training logs to output dir
        shutil.copy2(log_file, os.path.join(args.training_logs, "train.log"))

        # Write completion
        with open(os.path.join(args.training_logs, "done.txt"), "w") as f:
            f.write("Training complete.")

        EOF
    args:
      - --dataset_name
      - { inputValue: dataset_name }
      - --output_dir
      - { inputValue: output_dir }
      - --trained_model
      - { outputPath: trained_model }
      - --training_logs
      - { outputPath: training_logs }
