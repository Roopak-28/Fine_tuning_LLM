name: Fine-tune BERT with LoRA on Yelp Reviews
description: Fine-tunes a BERT model using Hugging Face Transformers and PEFT LoRA on Yelp Review Full dataset.
inputs:
  - name: dataset_name
    type: String
    description: Name of Hugging Face dataset (e.g. yelp_review_full)
  - name: model_uri
    type: String
    description: Pretrained Hugging Face model URI (e.g. bert-base-cased)
  - name: output_dir
    type: String
    description: Local output directory for Hugging Face Trainer
outputs:
  - name: trained_model
    type: Directory
    description: Directory containing the fine-tuned model
  - name: training_logs
    type: Directory
    description: Directory containing training logs
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install --quiet --no-cache-dir transformers datasets peft accelerate
        python3 -u -c "$0" "$@"  # Run inline Python
      - |
        import argparse
        import os
        import shutil
        from datasets import load_dataset
        from transformers import (
            AutoTokenizer,
            AutoModelForSequenceClassification,
            TrainingArguments,
            Trainer,
        )
        from peft import get_peft_model, LoraConfig, TaskType

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("--dataset_name", type=str, required=True)
        parser.add_argument("--model_uri", type=str, required=True)
        parser.add_argument("--output_dir", type=str, required=True)
        parser.add_argument("--trained_model", type=str, required=True)
        parser.add_argument("--training_logs", type=str, required=True)
        args = parser.parse_args()

        os.makedirs(args.output_dir, exist_ok=True)
        os.makedirs(args.trained_model, exist_ok=True)
        os.makedirs(args.training_logs, exist_ok=True)

        # Load dataset
        dataset = load_dataset(args.dataset_name, split="train[:3000]")
        dataset = dataset.rename_column("label", "labels")

        # Load tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(args.model_uri)
        model = AutoModelForSequenceClassification.from_pretrained(args.model_uri, num_labels=5)

        # Apply LoRA
        peft_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,
            r=8,
            lora_alpha=8,
            lora_dropout=0.1,
            bias="none",
        )
        model = get_peft_model(model, peft_config)

        # Tokenize
        def preprocess(example):
            return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

        dataset = dataset.map(preprocess, batched=True)

        # Training setup
        training_args = TrainingArguments(
            output_dir=args.output_dir,
            per_device_train_batch_size=8,
            num_train_epochs=1,
            logging_dir=os.path.join(args.output_dir, "logs"),
            logging_steps=10,
            save_strategy="no",
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
        )

        trainer.train()

        # Copy model output to trained_model dir
        for f in os.listdir(args.output_dir):
            src = os.path.join(args.output_dir, f)
            dst = os.path.join(args.trained_model, f)
            if os.path.isdir(src):
                shutil.copytree(src, dst, dirs_exist_ok=True)
            else:
                shutil.copy2(src, dst)

        # Write dummy log
        with open(os.path.join(args.training_logs, "log.txt"), "w") as f:
            f.write("Training complete.")
    args:
      - --dataset_name
      - { inputValue: dataset_name }
      - --model_uri
      - { inputValue: model_uri }
      - --output_dir
      - { inputValue: output_dir }
      - --trained_model
      - { outputPath: trained_model }
      - --training_logs
      - { outputPath: training_logs }
