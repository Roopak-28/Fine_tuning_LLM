name: Yelp Review Finetuning with PyTorch
description: |
  Train a Hugging Face Transformer model on the Yelp Review dataset using PyTorch and output the fine-tuned model.
inputs:
  - {
      name: model_name,
      type: String,
      default: distilbert-base-uncased,
      description: Hugging Face model checkpoint to fine-tune,
    }
  - {
      name: dataset_name,
      type: String,
      default: yelp_review_full,
      description: Hugging Face dataset identifier,
    }
  - {
      name: num_labels,
      type: Integer,
      default: 5,
      description: Number of classification labels,
    }
  - {
      name: max_length,
      type: Integer,
      default: 128,
      description: Maximum token length for input sequences,
    }
  - {
      name: num_epochs,
      type: Integer,
      default: 3,
      description: Number of training epochs,
    }
  - {
      name: train_batch_size,
      type: Integer,
      default: 16,
      description: Training batch size,
    }
  - {
      name: eval_batch_size,
      type: Integer,
      default: 16,
      description: Evaluation batch size,
    }
  - {
      name: output_dir,
      type: String,
      default: /mnt/models/yelp-output,
      description: Output directory to save fine-tuned model,
    }
outputs:
  - { name: model_output_path, type: String }
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location transformers datasets torch || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location transformers datasets torch --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import argparse
        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
        from datasets import load_dataset

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_name", type=str)
        parser.add_argument("--dataset_name", type=str)
        parser.add_argument("--num_labels", type=int)
        parser.add_argument("--max_length", type=int)
        parser.add_argument("--num_epochs", type=int)
        parser.add_argument("--train_batch_size", type=int)
        parser.add_argument("--eval_batch_size", type=int)
        parser.add_argument("--output_dir", type=str)
        parser.add_argument("--model_output_path", type=str)
        args = parser.parse_args()

        dataset = load_dataset(args.dataset_name)
        tokenizer = AutoTokenizer.from_pretrained(args.model_name)

        def tokenize(example):
            return tokenizer(example["text"], padding="max_length", truncation=True, max_length=args.max_length)

        dataset = dataset.map(tokenize, batched=True)
        dataset = dataset.rename_column("label", "labels")
        dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

        train_dataset = dataset["train"].shuffle(seed=42).select(range(1000))
        eval_dataset = dataset["test"].select(range(500))

        model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=args.num_labels)

        training_args = TrainingArguments(
            output_dir=args.output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.train_batch_size,
            per_device_eval_batch_size=args.eval_batch_size,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            logging_dir=os.path.join(args.output_dir, "logs"),
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )

        trainer.train()
        trainer.save_model(args.output_dir)

        with open(args.model_output_path, "w") as f:
            f.write(args.output_dir)
    args:
      - --model_name
      - { inputValue: model_name }
      - --dataset_name
      - { inputValue: dataset_name }
      - --num_labels
      - { inputValue: num_labels }
      - --max_length
      - { inputValue: max_length }
      - --num_epochs
      - { inputValue: num_epochs }
      - --train_batch_size
      - { inputValue: train_batch_size }
      - --eval_batch_size
      - { inputValue: eval_batch_size }
      - --output_dir
      - { inputValue: output_dir }
      - --model_output_path
      - { outputPath: model_output_path }
