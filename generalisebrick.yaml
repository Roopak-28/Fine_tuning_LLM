name: HuggingFace Yelp Review Finetune
description: |
  Fine-tune a HuggingFace model on the Yelp Review dataset using PyTorch.
  Saves the trained model to a mounted output directory.
metadata:
  annotations:
    pipelines.kubeflow.org/component_display_name: Yelp HuggingFace Trainer
    pipelines.kubeflow.org/component_description: |
      Downloads the Yelp Review dataset from HuggingFace Datasets,
      tokenizes it, fine-tunes a Transformer model, and saves the trained model.
inputs:
  - name: model_name
    type: String
    default: distilbert-base-uncased
    description: HuggingFace model to fine-tune
  - name: dataset_name
    type: String
    default: yelp_review_full
    description: Dataset identifier on HuggingFace hub
  - name: num_labels
    type: Integer
    default: 5
    description: Number of classification labels
  - name: max_length
    type: Integer
    default: 128
    description: Maximum token length
  - name: num_epochs
    type: Integer
    default: 3
    description: Number of training epochs
  - name: train_batch_size
    type: Integer
    default: 16
    description: Batch size for training
  - name: eval_batch_size
    type: Integer
    default: 16
    description: Batch size for evaluation
  - name: output_dir
    type: String
    default: /mnt/models/yelp
    description: Output directory path for saving model
outputs:
  - name: model_path
    type: String
    description: Path to saved trained model
implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - |
        pip install --quiet transformers datasets torch || pip install --user transformers datasets torch
        python3 -u -c "$0"
      - |
        import os
        import argparse
        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
        from datasets import load_dataset

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_name")
        parser.add_argument("--dataset_name")
        parser.add_argument("--num_labels", type=int)
        parser.add_argument("--max_length", type=int)
        parser.add_argument("--num_epochs", type=int)
        parser.add_argument("--train_batch_size", type=int)
        parser.add_argument("--eval_batch_size", type=int)
        parser.add_argument("--output_dir")
        parser.add_argument("--model_path")
        args = parser.parse_args()

        # Load and preprocess
        ds = load_dataset(args.dataset_name)
        tokenizer = AutoTokenizer.from_pretrained(args.model_name)
        def tokenize_fn(batch):
            return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=args.max_length)

        tokenized = ds.map(tokenize_fn, batched=True).rename_column("label", "labels")
        tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

        # Model + training
        model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=args.num_labels)
        training_args = TrainingArguments(
            output_dir=args.output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.train_batch_size,
            per_device_eval_batch_size=args.eval_batch_size,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            logging_dir=os.path.join(args.output_dir, "logs"),
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized["train"].shuffle(seed=42).select(range(1000)),
            eval_dataset=tokenized["test"].select(range(500)),
        )

        trainer.train()
        trainer.save_model(args.output_dir)

        os.makedirs(os.path.dirname(args.model_path), exist_ok=True)
        with open(args.model_path, "w") as f:
            f.write(args.output_dir)
    args:
      - --model_name
      - { inputValue: model_name }
      - --dataset_name
      - { inputValue: dataset_name }
      - --num_labels
      - { inputValue: num_labels }
      - --max_length
      - { inputValue: max_length }
      - --num_epochs
      - { inputValue: num_epochs }
      - --train_batch_size
      - { inputValue: train_batch_size }
      - --eval_batch_size
      - { inputValue: eval_batch_size }
      - --output_dir
      - { inputValue: output_dir }
      - --model_path
      - { outputPath: model_path }
