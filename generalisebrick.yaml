name: HuggingFace Yelp PytorchJob Trainer
description: |
  Trains a HuggingFace model on the Yelp Review dataset using Kubeflow's PyTorchJob operator with huggingface datasets.
metadata:
  annotations:
    pipelines.kubeflow.org/component_display_name: Yelp PytorchJob HuggingFace Trainer
    pipelines.kubeflow.org/component_description: |
      Fine-tunes any HuggingFace model on the Yelp dataset using PyTorchJob operator and saves model to PVC.
inputs:
  - { name: model_name, type: String, default: distilbert-base-uncased }
  - { name: dataset_name, type: String, default: yelp_review_full }
  - { name: num_labels, type: Integer, default: 5 }
  - { name: max_length, type: Integer, default: 128 }
  - { name: num_epochs, type: Integer, default: 3 }
  - { name: train_batch_size, type: Integer, default: 16 }
  - { name: eval_batch_size, type: Integer, default: 16 }
  - { name: output_model_dir, type: String, default: /mnt/models/yelp }
outputs:
  - { name: trained_model_path, type: String }
implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - |
        pip install --quiet torch transformers datasets kubeflow-training || pip install --user torch transformers datasets kubeflow-training
        python3 -u -c "$0"
      - |
        import os, argparse
        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
        from datasets import load_dataset
        from kubeflow.training import TrainingClient

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_name")
        parser.add_argument("--dataset_name")
        parser.add_argument("--num_labels", type=int)
        parser.add_argument("--max_length", type=int)
        parser.add_argument("--num_epochs", type=int)
        parser.add_argument("--train_batch_size", type=int)
        parser.add_argument("--eval_batch_size", type=int)
        parser.add_argument("--output_model_dir")
        parser.add_argument("--trained_model_path")
        args = parser.parse_args()

        def train():
            dataset = load_dataset(args.dataset_name)
            tokenizer = AutoTokenizer.from_pretrained(args.model_name)

            def preprocess(batch):
                return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=args.max_length)

            tokenized = dataset.map(preprocess, batched=True).rename_column("label", "labels")
            tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

            model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=args.num_labels)

            training_args = TrainingArguments(
                output_dir=args.output_model_dir,
                num_train_epochs=args.num_epochs,
                per_device_train_batch_size=args.train_batch_size,
                per_device_eval_batch_size=args.eval_batch_size,
                evaluation_strategy="epoch",
                save_strategy="epoch",
                logging_dir=os.path.join(args.output_model_dir, "logs"),
            )

            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=tokenized["train"].shuffle(seed=42).select(range(1000)),
                eval_dataset=tokenized["test"].select(range(500)),
            )

            trainer.train()
            trainer.save_model(args.output_model_dir)

        # Launch using PyTorchJob
        client = TrainingClient()
        client.create_job(
            name="huggingface-yelp-job",
            train_func=train,
            num_workers=1,
            num_procs_per_worker=1,
            resources_per_worker={"cpu": "4", "memory": "8Gi", "gpu": "1"}
        )

        os.makedirs(os.path.dirname(args.trained_model_path), exist_ok=True)
        with open(args.trained_model_path, "w") as f:
            f.write(args.output_model_dir)
    args:
      - --model_name
      - { inputValue: model_name }
      - --dataset_name
      - { inputValue: dataset_name }
      - --num_labels
      - { inputValue: num_labels }
      - --max_length
      - { inputValue: max_length }
      - --num_epochs
      - { inputValue: num_epochs }
      - --train_batch_size
      - { inputValue: train_batch_size }
      - --eval_batch_size
      - { inputValue: eval_batch_size }
      - --output_model_dir
      - { inputValue: output_model_dir }
      - --trained_model_path
      - { outputPath: trained_model_path }
